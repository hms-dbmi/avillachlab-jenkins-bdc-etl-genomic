<?xml version='1.1' encoding='UTF-8'?>
<flow-definition plugin="workflow-job@1360.vc6700e3136f5">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@2.2151.ve32c9d209a_3f"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@2.2151.ve32c9d209a_3f">
      <jobProperties/>
      <triggers/>
      <parameters>
        <string>DATASET1_S3_LOCATION</string>
        <string>TEMP_S3_LOCATION</string>
      </parameters>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>Instructions for new batch:&#xd;
In order to run this against a new batch first run the ETL - Merge Genomic Dataset job with two studies to prime the new batch.&#xd;
&#xd;
Once the new batch is created this pipeline can be run using a list of s3 locations in order to bulk load.  Currently only handling provisioning a single batch.</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>DATASET1_S3_LOCATION</name>
          <description>Static S3 location for dataset1</description>
          <defaultValue>s3://avillach-73-bdcatalyst-etl/general/hpds/genomics_javabins/9b/v4.1/releases/2024-07-23/batch4/</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>TEMP_S3_LOCATION</name>
          <description>Static S3 location for temporary data</description>
          <defaultValue>s3://avillach-73-bdcatalyst-etl/general/hpds/genomics_javabins/9b/v4.1/temp4/</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@3812.vc3031a_b_a_c955">
    <script>pipeline {
    agent any
    parameters {
        string(name: &apos;DATASET1_S3_LOCATION&apos;, defaultValue: &apos;s3://avillach-73-bdcatalyst-etl/general/hpds/genomics_javabins/9b/v4.1/releases/2024-07-23/batch4/&apos;, description: &apos;Static S3 location for dataset1&apos;)
        string(name: &apos;TEMP_S3_LOCATION&apos;, defaultValue: &apos;s3://avillach-73-bdcatalyst-etl/general/hpds/genomics_javabins/9b/v4.1/temp4/&apos;, description: &apos;Static S3 location for temporary data&apos;)
    }
    stages {
        stage(&apos;Trigger Batch File Generator&apos;) {
            steps {
                script {
                    // Trigger the Merge Batch File Generator job and capture the build number
                    def buildInfo = build job: &apos;Merge Batch File Generator&apos;, propagate: false
                    def buildNumber = buildInfo.getNumber()

                    // Wait for the build to complete and ensure it&apos;s successful
                    def buildResult = buildInfo.getResult()
                    if (buildResult != &apos;SUCCESS&apos;) {
                        error &quot;Failed to trigger Merge Batch File Generator job successfully.&quot;
                    }
                }
            }
        }
        stage(&apos;Read S3 Locations&apos;) {
            steps {
                script {
                    // Path to the artifact file
                    def artifactPath = &quot;${JENKINS_HOME}/workspace/Merge Batch File Generator/merge_batch_s3_locations.txt&quot;
                    def s3Locations = readFile(artifactPath).split(&apos;\n&apos;)

                    // Iterate over each location and trigger the ETL - Merge Genomic Dataset v4 job
                    s3Locations.each { location -&gt;
                        build job: &apos;ETL - Merge Genomic Dataset v4&apos;, parameters: [
                            string(name: &apos;dataset1_s3_location&apos;, value: params.DATASET1_S3_LOCATION),
                            string(name: &apos;dataset2_s3_location&apos;, value: location),
                            string(name: &apos;output_s3_location&apos;, value: params.DATASET1_S3_LOCATION),
                            string(name: &apos;temp_s3_location&apos;, value: params.TEMP_S3_LOCATION)
                        ]
                    }
                }
            }
        }
    }
}
</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <disabled>true</disabled>
</flow-definition>